// FIXME using an abstract provenance that was not declared does not lead to an immediate error
//  types need well-formedness checks
fn hist<s: prv, o: prv, gs:nat, ts:nat>(
    h_in: &s shrd cpu.mem [u8; gs*256*ts],
    h_out: &o uniq cpu.mem [u32; 256]
) -[t: cpu.thread]-> () {
    let mut gpu = gpu_device(0);

    let buffer = gpu_alloc_copy(&uniq gpu, &shrd *h_in);
    let mut histo = gpu_alloc_copy(&uniq gpu, &shrd *h_out);
    gpu_hist::<<<X<gs>, X<256>; [u32; 256]>>>(&shrd buffer, &uniq histo);
    copy_to_host(&shrd histo, h_out)
}

fn gpu_hist<gs:nat, ts:nat, r1: prv, r2: prv, r3: prv>(
    buffer: &r1 shrd gpu.global [u8; gs*256*ts],
    histo: &r2 uniq gpu.global [u32; 256],
    [grid.forall(X)] temp: &r3 uniq gpu.shared [u32; 256]
) -[grid: gpu.grid<X<gs>, X<256>>]-> () {
    let buffer_grouped = &shrd (*buffer).to_view.grp::<(gs*256)>.transp.grp::<256>;
    let histo_atomic = to_atomic_array(histo);
    sched block in grid {
        sched thread in block {
            (*temp).to_view[[thread]] = 0u32
        };
        sync;
        let atomic_temp = to_atomic_array(temp);
        sched thread in block {
            for i in 0..ts {
                let bucket = (*buffer_grouped)[[block]][[thread]][i];
                // TODO introduce index type idx[n] of all indices of maximum size n
                //  allow index expressions instead of nats for indexing
                atomic_fetch_add(&shrd (*atomic_temp)[bucket], 1u32)
            };
            sync(block); // only required for correctness (?)
            atomic_fetch_add(
                &shrd (*histo_atomic).to_view[[thread]],
                atomic_load(&shrd (*atomic_temp).to_view[[thread]])
            )
        }
    }
}

fn main() -[t:cpu.thread]-> () <'a, 'b>{
    let in_arr: &'a shrd cpu.mem [u8; 256*256*1024]  = unsafe _;
    let h_out: &'b uniq cpu.mem [u32; 256] = unsafe _;
    hist(in_arr, h_out)
}