fn prefix_scan(
    // TODO global fun params into same frame as top level declaration in body
    cpu_i_array: &uniq cpu.mem [i32; 64 * 1024 * 2],
    cpu_flag_array: &uniq cpu.mem [i32; 64],
    cpu_agg_array: &uniq cpu.mem [i32; 64],
    cpu_prefix_array: &uniq cpu.mem [i32; 64]
) -[cpu.thread]-> () {
    let mut gpu = gpu_device(0);

    let mut g_i = gpu_alloc_copy(&uniq gpu, &shrd *cpu_i_array);
    let mut g_flags = gpu_alloc_copy(&uniq gpu, &shrd *cpu_flag_array);
    let mut g_aggs = gpu_alloc_copy(&uniq gpu, &shrd *cpu_agg_array);
    let mut g_prefixs = gpu_alloc_copy(&uniq gpu, &shrd *cpu_prefix_array);
    exec::<64, 1024>(
        &uniq gpu,
        // FIXME nothing stops me from passing the complete @-value via a move here
        (
            &uniq g_i, 
            &uniq g_flags,
            &uniq g_aggs,
            &uniq g_prefixs
        ),
        | grid: BlockGrp<64, ThreadGrp<1024>>,
          input: (
              &uniq gpu.global [i32; 64 * 1024 * 2],
              &uniq gpu.global [i32; 64],
              &uniq gpu.global [i32; 64],
              &uniq gpu.global [i32; 64]
          )
        | -[gpu.grid]-> () {
            // TODO deconstruction works
            let gi_i = input.0;
            let gi_flags = input.1;
            let gi_aggs = input.2;
            let gi_prefixs = input.3;
            let view_a = group_mut::<2048>(to_view_mut(gi_i));
            let view_aggs = to_view_mut(gi_aggs);
            let view_flags = to_view_mut(gi_flags);
            let view_prefix = to_view_mut(gi_prefixs);
            decl {
                let mut shared_input: [i32; 1024 * 2] @ gpu.shared;
                let mut tmp2: [i32; 1024 * 2] @ gpu.shared;
                let mut result_reduction: [i32; 1024] @ gpu.shared;
                let mut exclusive_prefix: [i32; 1] @ gpu.shared
            } parfor block in grid 
                with a_group, agg, flag, prefix from view_a, view_aggs, view_flags, view_prefix {
                // Copy into shared
                let mut shared_input_view = group_mut::<2>(to_view_mut(& uniq shared_input));
                let mut result_view = to_view_mut(& uniq result_reduction);
                let global = group_mut::<2>(a_group);
                parfor thread in block
                with t_shared_input, f from
                    &uniq *shared_input_view, &uniq *global {
                    t_shared_input[0] = f[0];
                    t_shared_input[1] = f[1]
                };
                parfor thread in block
                with t_shared_input, r from &uniq *shared_input_view, &uniq *result_view {
                    *r = t_shared_input[0] + t_shared_input[1]
                };

                // borrowed from tree_reduce
                for_nat k in halved_range(512) {
                   let split_ib = (split uniq 2*k (*result_view)).0;
                   let (active_half0, active_half1) = split uniq k (*split_ib);

                   parfor thread in split_thread_grp::<k, 1024, 1, 1>(block).0
                   with fst_half, snd_half from active_half0, active_half1 {
                       *fst_half = *fst_half + *snd_half
                   }
                };

                parfor thread in split_thread_grp::<1>(block).0
                with r from &uniq *result_view {
                    // This feels wrong...
                    *agg = *r
                };
                // Cheesy Threadfence
                parfor thread in split_thread_grp::<1>(block).0
                with r from &uniq *result_view {
                    // This feels wrong...
                    *flag = 1
                };

                parfor thread in split_thread_grp::<1>(block).0
                with r from &uniq *result_view {
                    let unsafe_code_block_marker = 0
                // unsafe code block
                /* 
                  inline_cuda {
                      if (blockIdx.x > 0) {
                        bool done = false;
                        // WINDOW
                        int end_index = blockIdx.x - 5;
                        if (end_index < 0) end_index = 0;
                        while (!done) {
                            int i = blockIdx.x - 1;
                            while (i >= 0) {
                                int flag = gi_flags[i];
                                // printf("flag: %d\n", flag);
                                if (flag == 0) {
                                    break;
                                } else if (flag == 1) {
                                    exclusive_prefix[0] = exclusive_prefix[0] + gi_aggs[i];
                                } else if (flag == 2) {
                                    exclusive_prefix[0] = gi_prefixs[i] + exclusive_prefix[0];
                                    done = true;
                                    break;
                                }
                                i--;
                            }
                        }
                      }
                      __syncthreads();
                  }
                */
                };
                // Is this possible without with block? I do not need it?
                parfor thread in split_thread_grp::<1>(block).0
                with r from &uniq *result_view {
                    *prefix = *r
                };
                // Cheesy Threadfence
                parfor thread in split_thread_grp::<1>(block).0
                with r from &uniq *result_view {
                    *flag = 2
                };

                upsweep(block, &uniq shared_input);
                let tmp_view2 = to_view_mut(&uniq shared_input);
                let tmp_last = (split uniq 2047 (*tmp_view2)).1;
                parfor _ in split_thread_grp::<1>(block).0
                with last from tmp_last
                {
                    *last = 0
                };
                downsweep(block, &uniq shared_input);
                parfor thread in block
                with t_tmp, f from &uniq *shared_input_view, global {
                    f[0] = t_tmp[0] + exclusive_prefix[0];
                    f[1] = t_tmp[1] + exclusive_prefix[0]
                }
            }
        }
    );
    copy_to_host(&shrd g_i, cpu_i_array);
    copy_to_host(&shrd g_flags, cpu_flag_array);
    copy_to_host(&shrd g_aggs, cpu_agg_array);
    copy_to_host(&shrd g_prefixs, cpu_prefix_array)
}

fn upsweep(
    thread_grp: ThreadGrp<1024>,
    arr_ref: &uniq gpu.shared [i32; 2048]
) -[gpu.block]-> () <>{
    for_nat d in halved_range(1024) <'a>{
        let tmp_up_view = group_mut::<2048/d, 'a, gpu.shared, 2048, i32>(
            to_view_mut(&uniq *arr_ref));
        parfor _ in split_thread_grp::<d>(thread_grp).0
        with arr from tmp_up_view
        { arr[2048/d-1] = arr[2048/d-1] + arr[1024/d-1] }
    }
}

fn downsweep(
    block: ThreadGrp<1024>,
    arr_ref: &uniq gpu.shared [i32; 2048]
) -[gpu.block]-> () <>{
    for_nat d in doubled_range(1, 1024) <'b>{
        let tmp_down_view = group_mut::<2048 / d, 'b, gpu.shared, 2048, i32>(
            to_view_mut::<'b, gpu.shared, 2048, i32 > (&'b uniq *arr_ref));
        parfor _ in split_thread_grp::<d, 1024, 1, 1> (block).0
        with arr from tmp_down_view
        {
            let t = arr[1024/d-1];
            arr[1024/d-1] = arr[2048/d-1];
            arr[2048/d-1] = arr[2048/d-1] + t
        }
    }
}
