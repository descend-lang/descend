//#define TILE_SIZE 16 //so noch nicht möglich

// fn lud_descend<tile_size: nat, matrix_dim: nat, r:prv>(
//     m5: &r uniq cpu.mem [[f64; matrix_dim]; matrix_dim]
//     ) -[t: cpu.thread]-> ()
// {
//     let mut gpu = gpu_device(0);
//     let mut m_gpu = gpu_alloc_copy(&uniq gpu, &shrd *m5);
//
//     for_nat it in 0..(matrix_dim/tile_size-1) {
//
//         //1-D Grid mit 1-D Threads
//         lud_diagonal::<<<X<1>, X<tile_size>; [[[f64; tile_size]; tile_size]; 1]>>>::<it, tile_size, matrix_dim>(&uniq m_gpu);
//
//         lud_perimeter::<it, tile_size, matrix_dim>(&uniq m_gpu);
//
//         lud_internal::<it, tile_size, matrix_dim>(&uniq m_gpu)
//     };
//
//     lud_diagonal::<<<X<1>, X<tile_size>; [[[f64; tile_size]; tile_size]; 1]>>>::<(matrix_dim/tile_size)-1, tile_size, matrix_dim>(&uniq m_gpu); //::<(matrix_dim/tile_size) ,tile_size, matrix_dim>
//
//     copy_to_host(&shrd m_gpu, m5)
// }
//
//
// fn lud_diagonal<it:nat, tile_size: nat, matrix_dim: nat, r: prv, p: prv>(
//     m: &r uniq gpu.global [[f64; matrix_dim]; matrix_dim],
//     local_tile: &p uniq gpu.shared [[[f64; tile_size]; tile_size]; 1]
//     ) -[grid: gpu.grid<X<1>, X<tile_size>>]-> () <'a, 'e, 'f, 'g>
// {
//     //let tiles_per_dim = matrix_dim / tile_size;
//     let matrix_view: &'e uniq gpu.global [[ [[ [[ [[f64; tile_size]]; tile_size]]; (matrix_dim / tile_size)]]; (matrix_dim / tile_size) ]] = &'e uniq (*m).to_view.grp::<tile_size>.map::<[[ [f64; matrix_dim]; tile_size]]>(map::<[f64; matrix_dim]>(to_view).transp.grp::<tile_size>.map::<[[ [[f64; tile_size]]; tile_size]]>(transp)); //&uniq gpu.global [[ [[ [[ [[f64; tile_size]]; tile_size]]; tile s_per_dim]]; tiles_p er_dim]]
//
//     //Berechnung des Diagonal tiles
//     let row_of_tiles: &'f uniq gpu.global [[ [[ [[f64; tile_size]]; tile_size]]; (matrix_dim / tile_size) ]]= &'f uniq *matrix_view[it];
//     let position_of_tile: &'a uniq gpu.global [[ [[ [[f64; tile_size]]; tile_size]]; 1]] = &'a uniq (*row_of_tile)[..it..].1[..1..].0;
//
//     let local_tile_view: &'g uniq gpu.shared [[ [[ [[f64; tile_size]]; tile_size]]; 1]] = &'g uniq (*local_tile).to_view.map::<[ [f64; tile_size]; tile_size]>(to_view.map::<[f64; tile_size]>(to_view));
//
//     sched block in grid {// m: &uniq [[ [[f64; tile_size]]; tile_size]]
//         let tile = (*position_of_tile).0[[block]];
//         let local_tile_in_block =  local_tile_view[[block]];
//
//         diagonal_copy_to_local_mem::<tile_size>(tile, local_tile_in_block);
//         diagonal_block::<tile_size>(local_tile_in_block);
//         diagonal_copy_to_global_mem::<tile_size>(tile, local_tile_in_block)
//     }
// }
//
fn diagonal_copy_to_global_mem<tile_size: nat, a: prv, b: prv>(
    global_tile: &a uniq gpu.global [[ [[f64; tile_size]]; tile_size]],
    local_tile: &b uniq gpu.shared [[ [[f64; tile_size]]; tile_size]]
) -[block: gpu.block<X<tile_size>>]-> ()
{
    sched thread in block <'a, 'b> { //TODO kann so aliasing benutzt werden zur Optimierung?
        let global_thread_tile: &'a uniq gpu.global [[f64;tile_size]] = &'a uniq (*global_tile)[[thread]];
        let local_thread_tile: &'b uniq gpu.shared [[f64;tile_size]] = &'b uniq (*local_tile)[[thread]];

        for_nat i in 0..tile_size {
            (*global_thread_tile)[i] = (*local_thread_tile)[i]
        }
    }
}

fn diagonal_copy_to_local_mem<tile_size: nat, a: prv, b: prv>(
    global_tile: &a uniq gpu.global [[ [[f64; tile_size]]; tile_size]],
    local_tile: &b uniq gpu.shared [[ [[f64; tile_size]]; tile_size]]
    ) -[block: gpu.block<X<tile_size>>]-> ()
{

    sched thread in block <'a, 'b> { //TODO kann so aliasing benutzt werden zur Optimierung?
        let global_thread_tile: &'a uniq gpu.global [[f64;tile_size]] = &'a uniq (*global_tile)[[thread]];
        let local_thread_tile: &'b uniq gpu.shared [[f64;tile_size]] = &'b uniq (*local_tile)[[thread]];

        for_nat i in 0..tile_size {
            (*local_thread_tile)[i] = (*global_thread_tile)[i]
        }
    }
}


fn diagonal_block<tile_size: nat, a: prv>(
        m4: &a uniq gpu.shared [[ [[f64; tile_size]]; tile_size]]
    ) -[block: gpu.block<X<tile_size>>]-> ()
{
    for_nat i in 0..(tile_size-1) <'d, 'e, 'f, 'i, 'j, 'k, 'l, 'm, 'o, 'r> {
        //disjunkte partitionierung erstellen für threads
        let mat_view_partition: &'r uniq gpu.shared ([[ [[f64; tile_size]]; i+1]], [[ [[f64; tile_size]]; tile_size-(i+1)]]) = &'r uniq (*m4)[..i+1..]; //ab i+1-te Zeile abschneiden: (&shrd [[ [[f64; tile_size]]; i+1]], &shrd [[ [[f64; tile_size]]; tile_size - (i+1)]]) -> (Zeile 0..i, Zeile i+1...ende)

        // -> zugriff auf row.0 mit schleifenvariable i möglich für i-te Spalte für gemeinsamen Zugriff

        //1.transpose von row.1 -> für das Splitten an der richtigen Stelle
        //2.split -> so splitten das 0..i in dem 1. teil des tupels sind
        //3.transpose -> damit die Zeilen passend zu den thread_elementen auf die threads verteilt werden
        {
            let rhs_tile_transposed: & 'm uniq gpu.shared [[ [[f64; tile_size - (i +1)]]; tile_size]] = &'m uniq (*mat_view_partition).1.transp;
            //(split_columnl, split_columnr)
            let split_column: & 'f uniq gpu.shared  ([[ [[f64; tile_size - (i + 1)]]; i]], [[ [[f64; tile_size -(i + 1)]]; tile_size -i]]) = &'f uniq (*rhs_tile_transposed)[..i..];
            let thread_elements: & 'l uniq gpu.shared [[f64; tile_size - (i + 1)]] = &'l uniq (*split_column).1[0]
            //
            // //für indiviuelle Zugriffe: -> die nicht veränderbar sein sollen
            // let indiv_access: & 'd shrd gpu.shared [[ [[f64; i]]; tile_size - (i+ 1)]] = &'d shrd (*split_column).0.transp

            // indep(X) (tile_size-(i+1)) block { //elem: &uniq f64, shrd_row: &shrd [[f64, i]]
            //     active_threads => { () },
            //     inactive_threads => {
            //         sched thread in inactive_threads <'a, 'b, 'c, 'd> {
            //             let elem: &'a uniq gpu.shared f64 = &'a uniq (*thread_elements)[[thread]];
            //             let shrd_row: &'d uniq gpu.shared [[f64; i]] = &'d uniq (*indiv_access)[[thread]];
            //
            //             let row_i: &'b shrd gpu.shared [[f64; tile_size]] = &'b shrd (*mat_view_partition).0[i];
            //             let column_il: &'c shrd gpu.shared [[f64; i + 1]] = &'c shrd row_i[..i+1..].0;
            //             diagonal_below_HD::<i, a >(elem, shrd_row, column_il)
            //         }
            //     }
            // };
            // sync
        }
    //     //2. Berechnung
    //     //Elemente [i+1, i+1],..., [i+1, tile_size]
    //     let row_i1: &'o uniq gpu.shared [[f64; tile_size]] = &'o uniq (*mat_view_partition).1[0];
    //     let split_row_i1: &'e uniq gpu.shared ([[f64; i+1]], [[f64; tile_size-(i+1)]])  = &'e uniq (*row_i1)[..i+1..]; //-> (0..i, i+1..ende)
    //     let thread_elements: &'j uniq gpu.shared [[f64; tile_size-(i+1)]] = &'j uniq (*split_row_i1).1;
    //     let shrd_row: &'k shrd gpu.shared [[f64; i+1]] = &'k shrd (*split_row_i1).0;
    //
    //     //(shrd_row_access, thread_elements):
    //     let indiv_access: &'i shrd gpu.shared [[ [[f64; i+1]]; tile_size-(i+1)]]  = &'i shrd (*mat_view_partition).0.transp[..i+1..].1;
    //
    //     indep(X) (i+1) block  {
    //         active_threads => { () },
    //         inactive_threads => {
    //             sched thread in inactive_threads <'a, 'b, 'n> {
    //                 let elem: &'a uniq gpu.shared f64  = &'a uniq (*thread_elements)[[thread]];
    //                 let shrd_column: &'b uniq gpu.shared [[f64; i+1]] = &'a uniq (*indiv_access)[[thread]];
    //
    //                 diagonal_above_HD::<i, a>(elem, shrd_column, shrd_row)
    //             }
    //         }
    //     };
    //     sync
    }
}

fn diagonal_above_HD<i: nat, b: prv>(
    a: &b uniq gpu.shared f64, //verändender Zugriff
    shrd_column: &b shrd gpu.shared [[f64; i+1]],   //individuelle Zugriffe
    shrd_row: &b shrd gpu.shared [[f64; i+1]] //dies brauch jeder Thread zum Ausführen
    ) -[t: gpu.thread]-> ()
{
    for_nat j in 0..(i+1) {
        *a = *a - ((*shrd_row)[j] * (*shrd_column)[j])
    }
}

fn diagonal_below_HD<i: nat, b: prv>(
        a: &b uniq gpu.shared f64,
        shrd_row: &b shrd gpu.shared [[f64; i]],
        shrd_column: &b shrd gpu.shared [[f64; i+1]]  //i-te Spalte von row bekommen -> [[f64; i]] -> brauch jeder Thread
    ) -[t: gpu.thread]-> ()
{
    for_nat j in 0..i {
        *a = *a - ((*shrd_row)[j] * (*shrd_column)[j])
    };

    *a = *a / (*shrd_column)[i]
}



fn lud_perimeter<it:nat, tile_size: nat, matrix_dim: nat, r: prv>(
    m2: &r uniq gpu.global [[f64; matrix_dim]; matrix_dim]
    ) -[t: cpu.thread]-> ()
{
    //1-D Grid mit 1-D Threads
    //TODO Kopieren der Blöcke/Tiles in den shared gpu memory
        //TODO je nach Thread id Blöcke/Tiles oberhalb der HD (peri_row) oder unterhalb der HD (peri_col)
        //TODO diagonal Tile Kopieren
    //TODO Berechnung der Tiles -> unterschied zwischen peri_row und peri_col beachten
    //TODO zurück Kopieren in m
    let a = 1
}

fn lud_internal<it:nat, tile_size: nat, matrix_dim: nat, r:prv>(
    m3: &r uniq gpu.global [[f64; matrix_dim]; matrix_dim]
    ) -[t: cpu.thread]-> ()
{
    //2-D/1-D (beides möglich) grid mit 2-D Threads
    //TODO Kopieren der Blöcke/Tiles in den shared memory -> peri_col und peri_row
    //TODO summen Berechnung ausführen
    //TODO Berechnung in entsprechenden Eintrag von m schreiben-> pro Grid in einem
    let a = 1
}
