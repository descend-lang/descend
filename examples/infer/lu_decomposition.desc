//#define TILE_SIZE 16 //so noch nicht möglich

fn lud_descend<tile_size: nat, matrix_dim: nat, r:prv>(
    m5: &r uniq cpu.mem [[f32; matrix_dim]; matrix_dim]
    ) -[t: cpu.thread]-> ()
{
    let mut gpu = gpu_device(0);
    let mut m_gpu = gpu_alloc_copy(&uniq gpu, &shrd *m5);

    for_nat it in 0..(matrix_dim/tile_size-1) {

        //1-D Grid mit 1-D Threads
        lud_diagonal::<<<X<1>, X<tile_size>; [[[f32; tile_size]; tile_size]; 1]>>>::<it, tile_size, matrix_dim>(&uniq m_gpu);

       // lud_perimeter::<<<X<matrix_dim/tile_size-it-1>, X<tile_size*2>; [[[f32; tile_size]; tile_size]; (matrix_dim-it)/tile_size-1], [[[f32; tile_size]; tile_size]; (matrix_dim-it)/tile_size-1], [ [ [f32; tile_size]; tile_size];  matrix_dim/tile_size - it-1]>>>
         //           ::<it, tile_size, matrix_dim>(&uniq m_gpu);
        lud_perimeter::<it, tile_size, matrix_dim>(&uniq m_gpu);

        lud_internal::<it, tile_size, matrix_dim>(&uniq m_gpu)
    };

    lud_diagonal::<<<X<1>, X<tile_size>; [[[f32; tile_size]; tile_size]; 1]>>>::<(matrix_dim/tile_size)-1, tile_size, matrix_dim>(&uniq m_gpu); //::<(matrix_dim/tile_size) ,tile_size, matrix_dim>

    copy_to_host(&shrd m_gpu, m5)
}


fn lud_diagonal<it:nat, tile_size: nat, matrix_dim: nat, r: prv, p: prv>(
    m: &r uniq gpu.global [[f32; matrix_dim]; matrix_dim],
    local_tile: &p uniq gpu.shared [[[f32; tile_size]; tile_size]; 1]
    ) -[grid: gpu.grid<X<1>, X<tile_size>>]-> () <'a, 'e, 'f, 'g>
{
    //let tiles_per_dim = matrix_dim / tile_size;
    let matrix_view: &'e uniq gpu.global [[ [[ [[ [[f32; tile_size]]; tile_size]]; (matrix_dim / tile_size)]]; (matrix_dim / tile_size) ]] = &'e uniq (*m).to_view.grp::<tile_size>.map::<[[ [f32; matrix_dim]; tile_size]]>(map::<[f32; matrix_dim]>(to_view).transp.grp::<tile_size>.map::<[[ [[f32; tile_size]]; tile_size]]>(transp)); //&uniq gpu.global [[ [[ [[ [[f32; tile_size]]; tile_size]]; tile s_per_dim]]; tiles_p er_dim]]

    //Berechnung des Diagonal tiles
    let row_of_tiles: &'f uniq gpu.global [[ [[ [[f32; tile_size]]; tile_size]]; (matrix_dim / tile_size) ]]= &'f uniq (*matrix_view)[it];
    let position_of_tile: &'a uniq gpu.global [[ [[ [[f32; tile_size]]; tile_size]]; 1]] = &'a uniq (*row_of_tiles)[..it..].1[..1..].0;

    let local_tile_view: &'g uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]]; 1]] = &'g uniq (*local_tile).to_view.map::<[ [f32; tile_size]; tile_size]>(to_view.map::<[f32; tile_size]>(to_view));

    sched block in grid <'b, 'c> {// m: &uniq [[ [[f32; tile_size]]; tile_size]]
        let tile: &'c uniq gpu.global [[ [[f32; tile_size]]; tile_size]] = &'c uniq (*position_of_tile)[[block]];
        let local_tile_in_block: &'b uniq gpu.shared [[ [[f32; tile_size]]; tile_size]] =  &'b uniq (*local_tile_view)[[block]];

        diagonal_copy_to_local_mem::<tile_size>(&uniq *tile, &uniq *local_tile_in_block);
        diagonal_block::<tile_size>(&uniq *local_tile_in_block);
        diagonal_copy_to_global_mem::<tile_size>(tile, local_tile_in_block)
    }
}

fn diagonal_copy_to_global_mem<tile_size: nat, a: prv, b: prv>(
    global_tile: &a uniq gpu.global [[ [[f32; tile_size]]; tile_size]],
    local_tile: &b uniq gpu.shared [[ [[f32; tile_size]]; tile_size]]
) -[block: gpu.block<X<tile_size>>]-> ()
{
    sched thread in block <'a, 'b> { //TODO kann so aliasing benutzt werden zur Optimierung?
        let global_thread_tile: &'a uniq gpu.global [[f32;tile_size]] = &'a uniq (*global_tile)[[thread]];
        let local_thread_tile: &'b uniq gpu.shared [[f32;tile_size]] = &'b uniq (*local_tile)[[thread]];

        for_nat i in 0..tile_size {
            (*global_thread_tile)[i] = (*local_thread_tile)[i]
        }
    }
}

fn diagonal_copy_to_local_mem<tile_size: nat, a: prv, b: prv>(
    global_tile: &a uniq gpu.global [[ [[f32; tile_size]]; tile_size]],
    local_tile: &b uniq gpu.shared [[ [[f32; tile_size]]; tile_size]]
    ) -[block: gpu.block<X<tile_size>>]-> ()
{
    sched thread in block <'a, 'b> { //TODO kann so aliasing benutzt werden zur Optimierung?
        let global_thread_tile: &'a uniq gpu.global [[f32;tile_size]] = &'a uniq (*global_tile)[[thread]];
        let local_thread_tile: &'b uniq gpu.shared [[f32;tile_size]] = &'b uniq (*local_tile)[[thread]];

        for_nat i in 0..tile_size {
            (*local_thread_tile)[i] = (*global_thread_tile)[i]
        }
    }
}


fn diagonal_block<tile_size: nat, a: prv>(
        m4: &a uniq gpu.shared [[ [[f32; tile_size]]; tile_size]]
    ) -[block: gpu.block<X<tile_size>>]-> ()
{
    for_nat i in 0..(tile_size-1) <'d, 'e, 'f, 'i, 'j, 'k, 'l, 'm, 'o, 'r> {
        //disjunkte partitionierung erstellen für threads
        let mat_view_partition: &'r uniq gpu.shared ([[ [[f32; tile_size]]; i+1]], [[ [[f32; tile_size]]; tile_size-(i+1)]]) = &'r uniq (*m4)[..i+1..]; //ab i+1-te Zeile abschneiden: (&shrd [[ [[f32; tile_size]]; i+1]], &shrd [[ [[f32; tile_size]]; tile_size - (i+1)]]) -> (Zeile 0..i, Zeile i+1...ende)

        // -> zugriff auf row.0 mit schleifenvariable i möglich für i-te Spalte für gemeinsamen Zugriff

        //1.transpose von row.1 -> für das Splitten an der richtigen Stelle
        //2.split -> so splitten das 0..i in dem 1. teil des tupels sind
        //3.transpose -> damit die Zeilen passend zu den thread_elementen auf die threads verteilt werden
        {
            let rhs_tile_transposed: &'m uniq gpu.shared [[ [[f32; tile_size - (i +1)]]; tile_size]] = &'m uniq (*mat_view_partition).1.transp;

            let split_column: &'f uniq gpu.shared  ([[ [[f32; tile_size - (i + 1)]]; i]], [[ [[f32; tile_size -(i + 1)]]; tile_size -i]]) = &'f uniq (*rhs_tile_transposed)[..i..];
            let thread_elements: &'l uniq gpu.shared [[f32; tile_size - (i + 1)]] = &'l uniq (*split_column).1[0];

            //für indiviuelle Zugriffe: -> die nicht veränderbar sein sollen
            let indiv_access: &'d shrd gpu.shared [[ [[f32; i]]; tile_size - (i+ 1)]] = &'d shrd (*split_column).0.transp;

            indep(X) (tile_size-(i+1)) block { //elem: &uniq f32, shrd_row: &shrd [[f32, i]]
                active_threads => { () },
                inactive_threads => {
                    sched thread in inactive_threads <'a, 'b, 'c, 'd> {
                        let elem: &'a uniq gpu.shared f32 = &'a uniq (*thread_elements)[[thread]];
                        let shrd_row: &'d shrd gpu.shared [[f32; i]] = &'d shrd (*indiv_access)[[thread]];

                        let row_i: &'b shrd gpu.shared [[f32; tile_size]] = &'b shrd (*mat_view_partition).0[i];
                        let column_il: &'c shrd gpu.shared [[f32; i + 1]] = &'c shrd (*row_i)[..i+1..].0;
                        diagonal_below_HD::<i, a >(elem, shrd_row, column_il)
                    }
                }
            };
            sync
        };
        //2. Berechnung
        //Elemente [i+1, i+1],..., [i+1, tile_size]
        let row_i1: &'o uniq gpu.shared [[f32; tile_size]] = &'o uniq (*mat_view_partition).1[0];
        let split_row_i1: &'e uniq gpu.shared ([[f32; i+1]], [[f32; tile_size-(i+1)]])  = &'e uniq (*row_i1)[..i+1..]; //-> (0..i, i+1..ende)
        let thread_elements: &'j uniq gpu.shared [[f32; tile_size-(i+1)]] = &'j uniq (*split_row_i1).1;
        let shrd_row: &'k shrd gpu.shared [[f32; i+1]] = &'k shrd (*split_row_i1).0;

        //(shrd_row_access, thread_elements):
        let indiv_access: &'i shrd gpu.shared [[ [[f32; i+1]]; tile_size-(i+1)]]  = &'i shrd (*mat_view_partition).0.transp[..i+1..].1;

        indep(X) (i+1) block  {
            active_threads => { () },
            inactive_threads => {
                sched thread in inactive_threads <'a, 'b, 'n> {
                    let elem: &'a uniq gpu.shared f32  = &'a uniq (*thread_elements)[[thread]];
                    let shrd_column: &'b shrd gpu.shared [[f32; i+1]] = &'b shrd (*indiv_access)[[thread]];

                    diagonal_above_HD::<i, a>(elem, shrd_column, shrd_row)
                }
            }
        };
        sync
    }
}

fn diagonal_above_HD<i: nat, b: prv>(
    a: &b uniq gpu.shared f32, //verändender Zugriff
    shrd_column: &b shrd gpu.shared [[f32; i+1]],   //individuelle Zugriffe
    shrd_row: &b shrd gpu.shared [[f32; i+1]] //dies brauch jeder Thread zum Ausführen
    ) -[t: gpu.thread]-> ()
{
    for_nat j in 0..(i+1) {
        *a = *a - ((*shrd_row)[j] * (*shrd_column)[j])
    }
}

fn diagonal_below_HD<i: nat, b: prv>(
        a: &b uniq gpu.shared f32,
        shrd_row: &b shrd gpu.shared [[f32; i]],
        shrd_column: &b shrd gpu.shared [[f32; i+1]]  //i-te Spalte von row bekommen -> [[f32; i]] -> brauch jeder Thread
    ) -[t: gpu.thread]-> ()
{
    for_nat j in 0..i {
        *a = *a - ((*shrd_row)[j] * (*shrd_column)[j])
    };

    *a = *a / (*shrd_column)[i]
}



// fn lud_perimeter<it:nat, tile_size: nat, matrix_dim: nat, r: prv, a: prv, b: prv, c: prv>(
//     m2: &r uniq gpu.global [[f32; matrix_dim]; matrix_dim],
//     peri_row: &a uniq gpu.shared [[[f32; tile_size]; tile_size]; matrix_dim/tile_size-it-1],
//     peri_col: &b uniq gpu.shared [[[f32; tile_size]; tile_size]; matrix_dim/tile_size-it-1],
//     dia: &c uniq gpu.shared [[[f32; tile_size]; tile_size]; matrix_dim/tile_size - it-1]
//     ) -[grid: gpu.grid<X<matrix_dim/tile_size-it-1>, X<tile_size*2>>]-> () <'a, 'b, 'c, 'd, 'e, 'f, 'g>
// {
//     let matrix_view: &'a uniq gpu.global [[ [[ [[ [[f32; tile_size]]; tile_size]]; (matrix_dim / tile_size)]]; (matrix_dim / tile_size) ]] =
//         &'a uniq (*m2).to_view.grp::<tile_size>.map::<[[ [f32; matrix_dim]; tile_size]]>(map::<[f32; matrix_dim]>(to_view).transp.grp::<tile_size>.map::<[[ [[f32; tile_size]]; tile_size]]>(transp));
//
//     //Berechnung der Äußeren Tiles und des Diagonal-Tiles
//     //(row_of_tiles, rest) -> row_of_tiles for position of dia and peri_row, rest is for position of peri_col
//     let splitted_row_of_tiles_and_rest: &'b uniq gpu.global ([[ [[ [[ [[f32; tile_size]]; tile_size]]; (matrix_dim / tile_size) ]]; 1]], [[ [[ [[ [[f32; tile_size]]; tile_size]]; (matrix_dim / tile_size)]]; matrix_dim/tile_size -it-1]]) =
//         &'b uniq (*matrix_view)[..it..].1[..1..];
//     //(dia, peri_row) in global
//     let position_of_tile: &'c uniq gpu.global ([[ [[ [[f32; tile_size]]; tile_size]]; 1]], [[ [[ [[f32; tile_size]]; tile_size]]; matrix_dim/tile_size -it-1]])  =
//         &'c uniq (*splitted_row_of_tiles_and_rest).0[0][..it..].1[..1..];
//
//     //peri_col in global TODO merken für innere Tiles: (*splitted_row_of_tiles_and_rest).1.transp[..it..].1[..1..].1 noch zurück transponieren -> inneres tranpose rückgängig machen
//     let peri_col_global: &'d uniq gpu.global [[ [[ [[f32; tile_size]]; tile_size]]; matrix_dim/tile_size - it-1]] =
//         &'d uniq (*splitted_row_of_tiles_and_rest).1.transp[..it..].1[..1..].0[0];
//
//     //create shared views
//     let dia_view: &'e uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]]; matrix_dim/tile_size - it-1]] = &'e uniq (*dia).to_view.map::<[[f32; tile_size]; tile_size]>(to_view.map::<[f32; tile_size]>(to_view));
//     let peri_row_view: &'f uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]]; matrix_dim/tile_size - it-1]] = &'f uniq (*peri_row).to_view.map::<[[f32; tile_size]; tile_size]>(to_view.map::<[f32; tile_size]>(to_view));
//     let peri_col_view: &'g uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]]; matrix_dim/tile_size - it-1]] = &'g uniq (*peri_col).to_view.map::<[[f32; tile_size]; tile_size]>(to_view.map::<[f32; tile_size]>(to_view));
//
//     sched block in grid {
//         perimeter_copy_to_local_mem::< tile_size, matrix_dim > (&shrd (*position_of_tile).0[0], &uniq(*position_of_tile).1, &uniq *peri_col_global, &uniq *dia_view, &uniq *peri_row_view, &uniq *peri_col_view);
//
//         perimeter_block::< tile_size, matrix_dim > (dia_view, peri_row_view, peri_col_view);
//
//         perimeter_copy_to_global_mem::< tile_size, matrix_dim > (&uniq(*position_of_tile).1, &uniq *peri_col_global, &uniq *peri_row_view, &uniq *peri_col_view)
//     }
// }
//
//
// fn perimeter_block<tile_size: nat, matrix_dim: nat, a: prv, b:prv, c:prv>(
//     dia_shared: &a shrd gpu.shared [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     peri_row_shared: &b uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     peri_col_shared: &c uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]]
// )-[grid: gpu.grid<X<matrix_dim/tile_size-it-1>, X<tile_size*2>>]-> () <'a, 'b, 'c>
// {
//         let dia: &'a shrd gpu.shared [[ [[f32; tile_size]]; tile_size]] = &'a shrd (*dia_shared)[[block]];
//         let peri_row: &'b uniq gpu.shared [[ [[f32; tile_size]]; tile_size]] = &'b uniq (*peri_row_shared)[[block]].transp;
//         let peri_col: &'c uniq gpu.shared [[ [[f32; tile_size]]; tile_size]] = &'c uniq (*peri_col_shared)[[block]];
//
//         indep(X) tile_size block {
//             active_threads => {
//                 sched thread in active_threads <'d> {
//                     let peri_row_column: &'d uniq gpu.shared [[f32; tile_size]] = &'d uniq (*peri_row)[[thread]];
//                     calc_peri_row::<tile_size>(dia, peri_row_column)
//                 }
//             },
//             inactive_threads => {
//                 sched thread in inactive_threads <'e>{
//                     let peri_col_row: &'e uniq gpu.shared [[f32; tile_size]] = &'e uniq (*peri_col)[[thread]];
//                     calc_peri_col::<tile_size>(dia, peri_col_row)
//                 }
//             }
//         };
//         sync
// }
//
// fn calc_peri_row<tile_size: nat, a: prv, b: prv>(
//     dia: &a shrd gpu.shared [[ [[f32; tile_size]]; tile_size]],
//     peri_row: &b uniq gpu.shared [[f32; tile_size]]
// )-[t: gpu.thread]-> ()
// {
//     for_nat i in 1..tile_size {
//         for_nat j in 0..i {
//             (*peri_row)[i] = (*peri_row)[i] - (*dia)[i][j] * (*peri_row)[j]
//         }
//     }
// }
//
// fn calc_peri_col<tile_size: nat, a: prv, b: prv>(
//     dia: &a shrd gpu.shared [[ [[f32; tile_size]]; tile_size]],
//     peri_col: &b uniq gpu.shared [[f32; tile_size]]
// )-[t: gpu.thread]-> ()
// {
//     for_nat i in 1..tile_size {
//         for_nat j in 0..i {
//             (*peri_col)[i] = (*peri_col)[i] - (*dia)[i][j] * (*peri_col)[j]
//         };
//         (*peri_col)[i] = (*peri_col)[i] / (*dia)[i][i]
//     }
// }
//
//
// fn perimeter_copy_to_local_mem<it: nat, matrix_dim: nat, a: prv, b: prv, c: prv, d: prv, e: prv, f: prv>(
//     dia_global: &a shrd gpu.global [[ [[f32; tile_size]]; tile_size]],
//     peri_row_global: &b uniq gpu.global [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     peri_col_global: &c uniq gpu.global [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     dia_shared: &d uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     peri_row_shared: &e uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     peri_col_shared: &f uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]]
// ) -[grid: gpu.grid<X<matrix_dim/tile_size-it-1>, X<tile_size*2>>]-> ()
// {
//     sched block in grid <'a, 'b, 'c, 'd, 'e, 'f, 'g>{
//         let peri_row_global_tile: &'a uniq gpu.global [[ [[f32; tile_size]]; tile_size]] = &'a uniq (*peri_row_global)[[block]];
//         let peri_col_global_tile: &'b uniq gpu.global [[ [[f32; tile_size]]; tile_size]] = &'b uniq (*peri_col_global)[[block]];
//         let peri_row_shared_tile: &'c uniq gpu.shared [[ [[f32; tile_size]]; tile_size]] = &'c uniq (*peri_row_shared)[[block]];
//         let peri_col_shared_tile: &'d uniq gpu.shared [[ [[f32; tile_size]]; tile_size]] = &'d uniq (*peri_col_shared)[[block]];
//         let dia_tile: &'e uniq gpu.shared [[ [[f32; tile_size]]; tile_size]] = &'e uniq (*dia_shared)[[block]];
//
//         let split_dia_tile: &'f uniq gpu.shared ([[ [[f32; tile_size]]; tile_size/2]], [[ [[f32; tile_size]]; tile_size-tile_size/2]]) = &'f uniq (*dia_tile)[..tile_size/2..];
//         let split_dia_global: &'g shrd gpu.global ([[ [[f32; tile_size]]; tile_size/2]], [[ [[f32; tile_size]]; tile_size-tile_size/2]]) = &'g shrd (*dia_global)[..tile_size/2..];
//
//         indep(X) tile_size block {
//             active_threads => {
//                     sched thread in active_threads <'h, 'i, 'j, 'k> {
//                         let peri_row_global_tile_thread: &'h uniq gpu.global [[f32; tile_size]] = &'h uniq (*peri_row_global_tile)[[thread]];
//                         let peri_row_shared_tile_thread: &'i uniq gpu.shared [[f32; tile_size]] = &'i uniq (*peri_row_shared_tile)[[thread]];
//                         let dia_tile_thread: &'j uniq gpu.shared [[f32; tile_size/2]] = &'j uniq ((*split_dia_tile).0.transp)[[thread]];
//                         let dia_global_thread: &'k shrd gpu.global [[f32; tile_size/2]] = &'k shrd ((*split_dia_global).0.transp)[[thread]];
//
//                         for_nat i in 0..tile_size/2 {
//                             (*dia_tile_thread)[i] = (*dia_global_thread)[i]
//                         };
//
//                         for_nat i in 0..tile_size {
//                             (*peri_row_shared_tile_thread)[i] = (*peri_row_global_tile_thread)[i]
//                         }
//                     }
//                 },
//             inactive_threads => {
//                 sched thread in inactive_threads <'l, 'm, 'n, 'o> {
//                     let peri_col_global_tile_thread: &'l uniq gpu.global [[f32; tile_size]] = &'l uniq (*peri_col_global_tile)[[thread]];
//                     let peri_col_shared_tile_thread: &'m uniq gpu.shared [[f32; tile_size]] = &'m uniq (*peri_col_shared_tile)[[thread]];
//                     let dia_tile_thread: &'n uniq gpu.shared [[f32; tile_size-tile_size/2]] = &'n uniq ((*split_dia_tile).1.transp)[[thread]];
//                     let dia_global_thread: &'o shrd gpu.global [[f32; tile_size-tile_size/2]] = &'o shrd ((*split_dia_global).1.transp)[[thread]];
//
//                     for_nat i in 0..tile_size-tile_size/2 {
//                         (*dia_tile_thread)[i] = (*dia_global_thread)[i]
//                     };
//
//                     for_nat i in 0..tile_size {
//                         (*peri_col_shared_tile_thread)[i] = (*peri_col_global_tile_thread)[i]
//                     }
//                 }
//             }
//         };
//         sync
//     }
// }
//
// fn perimeter_copy_to_global_mem<it: nat, matrix_dim: nat, a: prv, b: prv, c: prv, d: prv, e: prv, f: prv>(
//     peri_row_global: &b uniq gpu.global [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     peri_col_global: &c uniq gpu.global [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     peri_row_shared: &e uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]],
//     peri_col_shared: &f uniq gpu.shared [[ [[ [[f32; tile_size]]; tile_size]];  matrix_dim/tile_size - it-1]]
// ) -[grid: gpu.grid<X<matrix_dim/tile_size-it-1>, X<tile_size*2>>]-> ()
// {
//     sched block in grid <'a, 'b, 'c, 'd>{
//         let peri_row_global_tile: &'a uniq gpu.global [[ [[f32; tile_size]]; tile_size]] = &'a uniq (*peri_row_global)[[block]];
//         let peri_col_global_tile: &'b uniq gpu.global [[ [[f32; tile_size]]; tile_size]] = &'b uniq (*peri_col_global)[[block]];
//         let peri_row_shared_tile: &'c uniq gpu.shared [[ [[f32; tile_size]]; tile_size]] = &'c uniq (*peri_row_shared)[[block]];
//         let peri_col_shared_tile: &'d uniq gpu.shared [[ [[f32; tile_size]]; tile_size]] = &'d uniq (*peri_col_shared)[[block]];
//
//         indep(X) tile_size block {
//             active_threads => {
//                 sched thread in active_threads <'e, 'f> {
//                     let peri_row_global_tile_thread: &'e uniq gpu.global [[f32; tile_size]] = &'e uniq (* peri_row_global_tile)[[thread]];
//                     let peri_row_shared_tile_thread: &'f uniq gpu.shared [[f32; tile_size]] = &'f uniq (* peri_row_shared_tile)[[thread]];
//
//                     for_nat i in 0..tile_size {
//                         (*peri_row_global_tile_thread)[i] = (*peri_row_shared_tile_thread)[i]
//                     }
//                 }
//             },
//             inactive_threads => {
//                 sched thread in inactive_threads <'g, 'h> {
//                     let peri_col_global_tile_thread: &'g uniq gpu.global [[f32; tile_size]] =  &'g uniq (*peri_col_global_tile)[[thread]];
//                     let peri_col_shared_tile_thread: &'h uniq gpu.shared [[f32; tile_size]] =  &'h uniq (*peri_col_shared_tile)[[thread]];
//
//                     for_nat i in 0..tile_size {
//                         (*peri_col_global_tile_thread)[i] = (*peri_col_shared_tile_thread)[i]
//                     }
//                 }
//             }
//         }
//
//     }
// }

fn lud_perimeter<it:nat, tile_size: nat, matrix_dim: nat, r: prv>(
    m2: &r uniq gpu.global [[f32; matrix_dim]; matrix_dim]
) -[t: cpu.thread]-> ()
{
    let a = 1
}
fn lud_internal<it:nat, tile_size: nat, matrix_dim: nat, r:prv>(
    m3: &r uniq gpu.global [[f32; matrix_dim]; matrix_dim]
    ) -[t: cpu.thread]-> ()
{
    //2-D/1-D (beides möglich) grid mit 2-D Threads
    //TODO Kopieren der Blöcke/Tiles in den shared memory -> peri_col und peri_row
    //TODO summen Berechnung ausführen
    //TODO Berechnung in entsprechenden Eintrag von m schreiben-> pro Grid in einem
    let a = 1
}
