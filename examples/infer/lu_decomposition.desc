//#define TILE_SIZE 16 //so noch nicht möglich

// fn lud_descend<tile_size: nat, matrix_dim: nat>(
//     m: &uniq cpu.mem [[f64; matrix_dim]; matrix_dim]
//     ) -[cpu.thread]-> ()
// {
//     let mut gpu = gpu_device(0);
//     let mut m_gpu = gpu_alloc_copy(&uniq gpu, &shrd *m);
//
//     for_nat it in 0..(matrix_dim/tile_size)-1 {
//
//         lud_diagonal::<it, tile_size, matrix_dim>(&uniq m_gpu, &uniq gpu);
//
//         lud_perimeter::<it, tile_size, matrix_dim>(&uniq m_gpu, &uniq gpu);
//
//         lud_internal::<it, tile_size, matrix_dim>(&uniq m_gpu, &uniq gpu)
//     };
//
//     lud_diagonal::<(matrix_dim/tile_size) ,tile_size, matrix_dim>(&uniq m_gpu, &uniq gpu);
//
//     copy_to_host(&shrd m_gpu, m)
// }
//
//
// fn lud_diagonal<it:nat, tile_size: nat, matrix_dim: nat>(
//     m: &uniq gpu.global [[f64; matrix_dim]; matrix_dim],
//     gpu: &uniq cpu.mem Gpu
//     ) -[cpu.thread]-> ()
// {
//     //1-D Grid mit 1-D Threads
//     exec::<1, tile_size>(
//         gpu,
//         (m,),
//         | grid: BlockGrp<1, ThreadGrp<tile_size>>, matrix: (
//                 &uniq gpu.global [[f64; matrix_dim]; matrix_dim]
//                 ) | -[gpu.grid]-> () {
//             let mut mat = matrix.0;
//             //let mat_dim = *matrix.1;
//             //let tile_size = *matrix.2; //als nat
//             //let it = *matrix.3; //als nat
//
//             //let tiles_per_dim = matrix_dim / tile_size;
//
//             //TODO in lokalen speicher laden -> bsp reduce_shared_mem (für 1. Version irrelevant)
//
//             let matrix_view = tile_partition::<tile_size, matrix_dim>(mat);//&uniq gpu.global [[ [[ [[ [[f64; tile_size]]; tile_size]]; tiles_per_dim]]; tiles_per_dim]]
//
//             //Berechnung des Diagonal tiles
//             let row_of_tiles = &uniq (*matrix_view)[it];
//             let position1_of_tile = split uniq (it-1) (*row_of_tiles);
//             let position_of_tile = split uniq 1 (*position1_of_tile).1;
//             //let splitted_row = split uniq it matrix_view
//             //let row_of_tile = split uniq 1 splitted_row.0
//
//             parfor block in grid with tile from &uniq position_of_tile.0 {// m: &uniq [[ [[f64; tile_size]]; tile_size]] TODO -> umschreiben mit split
//                 diagonal_block(tile)
//             }
//         }
//     )
// }


fn diagonal_block<tile_size: nat, a: prv>(
        m: &a uniq gpu.global [[ [[f64; tile_size]]; tile_size]],
        block: ThreadGrp<tile_size>
    ) -[gpu.block]-> ()
{
    for_nat i in 0..tile_size-1 <'d, 'e, 'f, 'g, 'h, 'i, 'j, 'k, 'l, 'm, 'n, 'o, 'r, 's> {
        //disjunkte partitionierung erstellen für threads
        let (lhs, rhs): (&'r uniq gpu.global [[ [[f64; tile_size]]; i+1]], &'s uniq gpu.global [[ [[f64; tile_size]]; tile_size-(i+1)]]) =
            split 'r 's uniq (i + 1) (*m); //ab i+1-te Zeile abschneiden: (&shrd [[ [[f64; tile_size]]; i+1]], &shrd [[ [[f64; tile_size]]; tile_size - (i+1)]]) -> (Zeile 0..i, Zeile i+1...ende)


        // -> zugriff auf row.0 mit schleifenvariable i möglich für i-te Spalte für gemeinsamen Zugriff

        //1.transpose von row.1 -> für das Splitten an der richtigen Stelle
        //2.split -> so splitten das 0..i in dem 1. teil des tupels sind
        //3.transpose -> damit die Zeilen passend zu den thread_elementen auf die threads verteilt werden
        let rhs_tile_transposed = transpose_mut(&uniq *rhs);
        let (split_columnl, split_columnr): (&'f uniq gpu.global [[ [[f64; tile_size-(i+1)]]; i]], &'g uniq gpu.global [[ [[f64; tile_size-(i+1)]]; tile_size-i]]) =
            split 'f 'g uniq i (*rhs_tile_transposed);
        let thread_elements: &'l uniq gpu.global [[f64; tile_size-(i+1)]] =  &'l uniq *split_columnr[0];
        //let thread_elements = transpose_mut(split_columnl0); //[[f64; tile_size-1-i]]

        //für indiviuelle Zugriffe: -> die nicht veränderbar sein sollen
        let indiv_access: &'d shrd gpu.global [[ [[f64; i]]; tile_size-(i+1)]] = transpose::<'d>(&'d shrd *split_columnl);

        //let (mut_elements, shrd_elements) = split uniq tile_size to_view_mut(array)
        parfor thread in block with elem, shrd_row from thread_elements, indiv_access <'b, 'c, 'd>{ //elem: &uniq f64, shrd_row: &shrd [[f64, i]]
            let row_i: &'b shrd gpu.global [[f64; tile_size]] = &'b shrd *lhs[i];
            let (row_il, row_ir): (&'c shrd gpu.global [[f64; i]], &'d shrd gpu.global [[f64; tile_size-i]]) = split 'c 'd shrd i (*row_i);
            diagonal_below_HD::<i, a>(elem, shrd_row, row_il)
        }; //sync am ende von parfor

        //2. Berechnung
        //Elemente [i+1, i+1],..., [i+1, tile_size]
        let row_i1: &'o uniq gpu.global [[f64; tile_size]] = &'o uniq *rhs[0];
        let (shrd_row_access, thread_elements): (&'e uniq gpu.global [[f64; i+1]], &'h uniq gpu.global [[f64; tile_size-(i+1)]])  = split 'e 'h uniq (i + 1) (*row_i1); // &uniq ([[f64; i+1]], [[f64, tile_size-(i+1)]] -> (0..i, i+1..ende)
        //let thread_elements = &uniq *split_row_i1r; //&uniq [[f64, tile_size-(i+1]]

        let row_i0_transpose: &'i shrd gpu.global [[ [[f64; i+1]]; tile_size]]  = transpose::<'i>(&'i shrd *lhs);
        let (split_row_i0_columnl, split_row_i0_columnr): (&'j shrd gpu.global [[ [[f64; (i+1)]]; (i+1)]], &'k shrd gpu.global [[ [[f64; (i+1)]]; tile_size-(i+1)]]) =
            split 'j 'k shrd (i+1) (*row_i0_transpose);
        let indiv_access: &'m shrd gpu.global [[ [[f64; tile_size-(i+1)]]; (i+1)]]  = transpose::<'m>(&'m shrd *split_row_i0_columnr);

        parfor thread in block with elem, shrd_column from thread_elements, indiv_access {
            let shrd_row: &'n shrd gpu.global [[f64; i+1]]  = &'n shrd *shrd_row_access; //&shrd [[f64; i+1]]
            diagonal_above_HD::<i, a>(elem, shrd_column, shrd_row)
        }
    }
}

fn diagonal_above_HD<i: nat, b: prv>(
    a: &b uniq gpu.global f64, //verändender Zugriff
    shrd_column: &b shrd gpu.global [[f64; i]],   //individuelle Zugriffe
    shrd_row: &b shrd gpu.global [[f64; i]] //dies brauch jeder Thread zum Ausführen
    ) -[gpu.thread]-> ()
{
    for_nat j in 0..i {
        *a = *a - (*shrd_row[j] * *shrd_column[j])
    }
}

fn diagonal_below_HD<i: nat, b: prv>(
        a: &b uniq gpu.global f64,
        shrd_row: &b shrd gpu.global [[f64; i]],
        shrd_column: &b shrd gpu.global [[f64; i]]  //i-te Spalte von row bekommen -> [[f64; i]] -> brauch jeder Thread
    ) -[gpu.thread]-> ()
{
    for_nat j in 0..i-1 {
        *a = *a - (*shrd_row[j] * *shrd_column[j])
    };

    *a = *a / *shrd_column[i]
}


fn tile_partition<tile_size: nat, matrix_dim: nat, a: prv>(
    m: &a uniq gpu.global [[f64; matrix_dim]; matrix_dim]
    ) -[view]-> &a uniq gpu.global [[ [[ [[ [[f64; tile_size]]; tile_size]]; (matrix_dim / tile_size)]]; (matrix_dim / tile_size)]] //im parser eingeführt
{
    let inner_array_to_view = |f: &a uniq gpu.global [f64; matrix_dim]| -[view]-> &a uniq gpu.global [[f64; matrix_dim]] { to_view_mut::<a>(f) };
    let lambda = |x: &a uniq gpu.global [[ [[f64; tile_size]]; tile_size]]| -[view]-> &a uniq gpu.global [[ [[f64; tile_size]]; tile_size]] { transpose_mut::<a>(x) };
    let outter_lambda = |g: &a uniq gpu.global [[ [f64; matrix_dim]; tile_size]]| -[view]-> &a uniq gpu.global [[ [[ [[f64; tile_size]]; tile_size]]; (matrix_dim / tile_size)]]
        {map_mut::<a>(lambda, //&uniq gpu.global [[ [[ [[ [[f64; tile_size]]; tile_size]]; tile_per_dim]]; tile_per_dim]]
            group_mut::<tile_size, a>( //&uniq gpu.global [[ [[ [[f64; tile_size]]; tile_size]]; tile_per_dim]]
                transpose_mut::<a>(map_mut::<a>(inner_array_to_view, g))) //&uniq gpu.global [[ [[ [[f64; tile_size]]; matrix_dim]]; tiles_per_dim]
        )};
    //map_mut:<r1: prv, d: dty, d2: dty, m: mem, n: nat>(lambda: |&r1 uniq d| -[view]-> d2, &r1 uniq m [[d;n]]) -[view]-> &r1 uniq m [[d2; n]]
    map_mut::<a>(outter_lambda,
        group_mut::<tile_size, a>(to_view_mut::<a>(m)) //&uniq gpu.global [[ [[ [f64; matrix_dim]; tile_size]]; tiles_per_dim]]
    )
}


fn lud_perimeter<it:nat, tile_size: nat, matrix_dim: nat>(
    m: &uniq gpu.global [[f64; matrix_dim]; matrix_dim],
    gpu: &uniq cpu.mem Gpu
    ) -[cpu.thread]-> ()
{
    //1-D Grid mit 1-D Threads
    //TODO Kopieren der Blöcke/Tiles in den shared gpu memory
        //TODO je nach Thread id Blöcke/Tiles oberhalb der HD (peri_row) oder unterhalb der HD (peri_col)
        //TODO diagonal Tile Kopieren
    //TODO Berechnung der Tiles -> unterschied zwischen peri_row und peri_col beachten
    //TODO zurück Kopieren in m
    let a = 1
}

fn lud_internal<it:nat, tile_size: nat, matrix_dim: nat>(
    m: &uniq gpu.global [[f64; matrix_dim]; matrix_dim],
    gpu: &uniq cpu.mem Gpu
    ) -[cpu.thread]-> ()
{
    //2-D/1-D (beides möglich) grid mit 2-D Threads
    //TODO Kopieren der Blöcke/Tiles in den shared memory -> peri_col und peri_row
    //TODO summen Berechnung ausführen
    //TODO Berechnung in entsprechenden Eintrag von m schreiben-> pro Grid in einem
    let a = 1
}
